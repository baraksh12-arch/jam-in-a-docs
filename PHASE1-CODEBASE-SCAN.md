# Phase 1: Codebase Scan & Mapping Report
**Date:** Current Session  
**Status:** âœ… Complete - Ready for Phase 2

---

## ğŸ“‹ Executive Summary

The Jam-in-a-Doc codebase is a React-based collaborative music application using:
- **Raw Web Audio API** (no Tone.js currently)
- **WebRTC DataChannels** for peer-to-peer communication
- **Supabase** for room/player state management
- **Custom clock synchronization** for room time
- **Event bundling** for non-drum instruments

---

## ğŸ¹ Instrument Architecture

### Current Implementation

#### 1. **Drums** (`DRUMS`)
- **Location:** `src/components/instruments/DrumPad.jsx`
- **Trigger:** `onNotePlay(padId)` â†’ calls `audioEngine.playNote('DRUMS', padId)`
- **Sound Generation:** `useAudioEngine.jsx` â†’ `playDrumSound(drumType, when)`
- **Synthesis Method:** Raw Web Audio API with oscillators + noise buffers
  - Kick: Dual oscillators (150Hz â†’ 40Hz, 80Hz â†’ 30Hz)
  - Snare: Triangle oscillator + filtered white noise
  - Hi-hat: Filtered white noise (highpass @ 7kHz)
  - Crash: Long filtered noise (1.5s decay)
  - Toms: Oscillators with frequency sweeps
- **Polyphony Management:** Voice stealing system (max 32 simultaneous voices)
- **Latency Handling:** 
  - **Sending:** Bypasses bundler, sends immediately via WebRTC
  - **Receiving:** Plays immediately, no scheduling (ultra-low latency path)

#### 2. **Bass** (`BASS`)
- **Location:** `src/components/instruments/PianoKeyboard.jsx` (shared with EP/GUITAR)
- **Trigger:** `onNotePlay(midiNote)` â†’ calls `audioEngine.playNote('BASS', midiNote)`
- **Sound Generation:** `useAudioEngine.jsx` â†’ `playBassSynth(frequency, duration, when)`
- **Synthesis Method:** Raw Web Audio API
  - 3 oscillators: sawtooth (fundamental), square (sub-oscillator), sawtooth (detuned)
  - Lowpass filter with frequency sweep
  - ADSR envelope (attack: 0.001s ULTRA / 0.01s SYNCED)
- **Note Range:** MIDI 28-52 (E1 to E3)

#### 3. **Electric Piano** (`EP`)
- **Location:** `src/components/instruments/PianoKeyboard.jsx`
- **Trigger:** `onNotePlay(midiNote)` â†’ calls `audioEngine.playNote('EP', midiNote)`
- **Sound Generation:** `useAudioEngine.jsx` â†’ `playEPianoSynth(frequency, duration, when)`
- **Synthesis Method:** Raw Web Audio API (FM synthesis)
  - Carrier: sine wave
  - Modulator: sine wave at 3.5x frequency
  - Lowpass filter
  - ADSR envelope (attack: 0.001s ULTRA / 0.01s SYNCED)
- **Note Range:** MIDI 48-84 (C3 to C6)

#### 4. **Guitar** (`GUITAR`)
- **Location:** `src/components/instruments/PianoKeyboard.jsx`
- **Trigger:** `onNotePlay(midiNote)` â†’ calls `audioEngine.playNote('GUITAR', midiNote)`
- **Sound Generation:** `useAudioEngine.jsx` â†’ `playGuitarSynth(frequency, duration, when)`
- **Synthesis Method:** Raw Web Audio API (Karplus-Strong inspired)
  - 3 oscillators: sawtooth, square (2x), triangle (0.5x)
  - WaveShaper distortion (tanh curve)
  - Lowpass filter with frequency sweep
  - ADSR envelope (attack: 0.001s ULTRA / 0.005s SYNCED)
- **Note Range:** MIDI 40-76 (E2 to E5)

### Audio Engine Hook
**File:** `src/components/hooks/useAudioEngine.jsx`

**Key Functions:**
- `playNote(instrument, note, velocity)` - Immediate playback
- `playNoteAt(instrument, note, velocity, whenInSeconds)` - Scheduled playback
- `stopNote(instrument, note)` - Stop note
- `stopNoteAt(instrument, note, whenInSeconds)` - Scheduled stop
- `setInstrumentVolume(instrument, value)` - Volume control
- `getAudioContext()` - Returns AudioContext for scheduling

**Current State:**
- âœ… Uses raw Web Audio API (AudioContext, Oscillators, GainNodes, Filters)
- âœ… Supports scheduled playback via `playNoteAt()`
- âœ… Has warmup mechanism for ULTRA_LOW_LATENCY mode
- âŒ **No Tone.js integration**
- âŒ **No sample-based instruments** (all procedural synthesis)

---

## ğŸŒ Network & Event Flow

### Note Event Sending
**Flow:** `InstrumentPanel` â†’ `sendNote()` â†’ `useNoteEvents.sendNote()` â†’ `webrtc.sendJamEvent()` â†’ `WebRTCManager.sendJamEvent()`

**File:** `src/components/hooks/useNoteEvents.jsx`
- Creates jam event with `roomTime` from `webrtc.getRoomTime()`
- Sends via `webrtc.sendJamEvent(event)`
- **DRUMS:** Bypasses bundler, sends immediately
- **Other instruments:** Queued in bundler (8ms ULTRA / 16ms SYNCED intervals)

**File:** `src/lib/webrtcManager.js`
- `sendJamEvent(event)` routes to bundler or immediate send
- `sendBundle(eventsArray)` serializes and sends to all connected peers
- Uses unordered, unreliable DataChannels (`ordered: false, maxRetransmits: 0`)

### Note Event Receiving
**Flow:** WebRTC DataChannel â†’ `WebRTCManager.onmessage` â†’ `useNoteEvents` listener â†’ `audioEngine.playNote()` or `playNoteAt()`

**File:** `src/components/hooks/useNoteEvents.jsx`
- Listens to `webrtc.onJamEvent(callback)`
- **DRUMS:** Always plays immediately (bypasses all scheduling)
- **ULTRA mode:** Non-drums play immediately
- **SYNCED mode:** Non-drums use `computeTargetAudioTime()` for scheduling

**Scheduling Logic:**
```javascript
targetAudioTime = audioContext.currentTime + timeDelta + latencySeconds + safetySeconds
```
- `timeDelta = roomTimeFromEvent - currentRoomTime`
- `latencySeconds = peerLatencyMs / 1000`
- `safetySeconds = 1.0ms / 1000` (SAFETY_OFFSET_MS)

---

## â±ï¸ Clock Synchronization

### Current Implementation
**File:** `src/lib/clockSync.js`

**ClockSync Class:**
- **Room Time:** Based on `rooms.created_at` timestamp from Supabase
- **Latency Measurement:** Ping-pong RTT measurement (every 500ms per peer)
- **Latency Estimation:** 
  - Median of last 5 RTTs (spike rejection: >2x median)
  - Kalman-like filter for smoothing
  - One-way latency = RTT / 2

**Key Methods:**
- `getRoomTime()` - Returns seconds since room start
- `computeTargetAudioTime(roomTimeFromMessage, audioContext, peerId)` - Calculates when to play note
- `updateLatency(peerId, rttMs)` - Updates latency estimate

**Limitations:**
- âŒ **No server-side clock sync** (relies on Supabase timestamp)
- âŒ **No time offset calculation** (no `timeOffset = serverTime - clientTime`)
- âŒ **No periodic clock sync updates** (only initial room timestamp)
- âœ… Has latency compensation per peer
- âœ… Has safety offset (1.0ms)

---

## ğŸ“¦ Event Bundling

**File:** `src/lib/jamEventBundler.js`

**JamEventBundler Class:**
- Queues events and flushes at intervals
- **ULTRA mode:** 8ms flush interval (~125 fps)
- **SYNCED mode:** 16ms flush interval (~60 fps)
- **DRUMS:** Always bypass bundler (immediate send)

**Bundle Format:**
- Single event: `{ type, instrument, note, ... }`
- Multiple events: `{ kind: 'bundle', events: [...] }`

---

## ğŸ”Œ WebRTC Architecture

**File:** `src/lib/webrtcManager.js`

**Connection Model:**
- Full mesh topology (each player connects to all others)
- One DataChannel per peer connection (named "midi")
- Unordered, unreliable channels for lowest latency

**Key Features:**
- Automatic peer connection management
- Ping/pong for latency measurement
- Event bundling integration
- Connection state tracking

**File:** `src/components/hooks/useWebRTC.jsx`
- React hook wrapper for WebRTCManager
- Manages peer lifecycle (add/remove based on `useRoomState.peers`)
- Exposes `sendJamEvent()`, `onJamEvent()`, `getRoomTime()`, `computeTargetAudioTime()`

---

## ğŸ›ï¸ Instrument Claiming

**File:** `src/components/hooks/useRoomState.jsx`

**Current Flow:**
1. User clicks "Claim Instrument" â†’ `claimMyInstrument(instrument)`
2. Calls `claimInstrument(roomId, userId, instrument)` (Firebase/Supabase)
3. Room state updates via subscription â†’ `players` array updates
4. `useWebRTC` detects peer changes â†’ adds/removes WebRTC connections
5. UI updates to show claimed instrument

**Limitations:**
- âŒ **No silent peer refresh** - WebRTC connections are recreated
- âŒ **No broadcast of claim events** - Relies on database subscription
- âœ… Instrument state persists in database
- âœ… Players can see who has which instrument

---

## ğŸšï¸ Latency Modes

**File:** `src/config/latencyMode.js`

**Current Modes:**
1. **ULTRA_LOW_LATENCY** (currently active)
   - DRUMS: Immediate send/receive
   - Other instruments: Immediate playback (no scheduling)
   - Bundle interval: 8ms

2. **SYNCED**
   - DRUMS: Immediate send/receive
   - Other instruments: Clock-synchronized scheduling with latency compensation
   - Bundle interval: 16ms

**Note:** Mode is hardcoded to `ULTRA` globally (no UI toggle yet)

---

## ğŸ“Š Current Architecture Strengths

âœ… **Ultra-low latency path for drums** (bypasses bundler and scheduling)  
âœ… **WebRTC peer-to-peer** (no server bottleneck)  
âœ… **Unreliable DataChannels** (drops late packets, prevents audio glitches)  
âœ… **Polyphony management for drums** (voice stealing)  
âœ… **Latency measurement per peer** (ping-pong with Kalman filtering)  
âœ… **Event bundling** (reduces burst pressure)  
âœ… **Scheduled playback support** (via `playNoteAt()`)  

---

## ğŸš¨ Current Architecture Gaps (vs Google Shared Piano)

### Audio Engine
âŒ **No Tone.js** - Using raw Web Audio API (more complex, less optimized)  
âŒ **No sample-based instruments** - All procedural synthesis  
âŒ **No Tone.Transport** - No unified timing system  
âŒ **No preloaded samples** - Samples generated on-the-fly  

### Clock Sync
âŒ **No server-side clock sync** - Only uses room creation timestamp  
âŒ **No time offset calculation** - No `timeOffset = serverTime - clientTime`  
âŒ **No periodic sync updates** - Only initial timestamp  
âŒ **No shared clock** - Each client calculates room time independently  

### Scheduling
âŒ **No global latency buffer** - Uses per-peer latency + 1ms safety  
âŒ **No late note filtering** - Plays notes even if they arrive late  
âŒ **No jitter handling** - No sliding window or deduplication beyond basic Set  
âŒ **No timestamp-based scheduling** - Uses room time delta, not absolute timestamps  

### Instrument Claiming
âŒ **No silent peer refresh** - WebRTC connections are recreated on claim  
âŒ **No claim event broadcast** - Relies on database subscription (slower)  

---

## ğŸ“ Key Files Reference

### Audio
- `src/components/hooks/useAudioEngine.jsx` - Audio engine (Web Audio API)
- `src/components/instruments/DrumPad.jsx` - Drum pad UI
- `src/components/instruments/PianoKeyboard.jsx` - Piano keyboard UI (shared)

### Networking
- `src/lib/webrtcManager.js` - WebRTC connection management
- `src/lib/webrtcSignaling.js` - WebRTC signaling (not shown, but referenced)
- `src/components/hooks/useWebRTC.jsx` - React hook for WebRTC
- `src/components/hooks/useNoteEvents.jsx` - Note event sending/receiving

### Synchronization
- `src/lib/clockSync.js` - Clock sync and latency estimation
- `src/lib/jamEventProtocol.js` - Event serialization
- `src/lib/jamEventBundler.js` - Event bundling

### State Management
- `src/components/hooks/useRoomState.jsx` - Room and player state
- `src/config/latencyMode.js` - Latency mode configuration

### UI
- `src/pages/Room.jsx` - Main room page
- `src/components/InstrumentPanel.jsx` - Instrument panel UI
- `src/components/InstrumentGrid.jsx` - Grid of instrument panels

---

## ğŸ¯ Phase 2 Readiness

**Status:** âœ… Ready to proceed

**Prerequisites Met:**
- âœ… All instrument modules identified
- âœ… Current trigger logic mapped
- âœ… Network flow understood
- âœ… Clock sync mechanism documented
- âœ… Scheduling logic analyzed

**Next Steps (Phase 2):**
1. Install Tone.js dependency
2. Create modular Tone.js instrument modules
3. Replace `useAudioEngine` with Tone.js-based implementation
4. Ensure backward compatibility with existing trigger logic

---

**End of Phase 1 Report**

